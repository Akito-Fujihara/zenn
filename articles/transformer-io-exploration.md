---
title: "GPT-2 ã§ç†è§£ã™ã‚‹ Transformer ã®å…¥å‡ºåŠ› - ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‹ã‚‰èª­ã¿è§£ãä»•çµ„ã¿"
emoji: "ğŸ¤–"
type: "tech"
topics: ["transformer", "gpt2", "nlp", "python", "huggingface"]
published: true
---

# ã¯ã˜ã‚ã«

ã“ã®è¨˜äº‹ã§ã¯ã€OpenAI ã® **GPT-2** ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã€Transformer ã®å…¥å‡ºåŠ›ã‚’ **ãƒ‡ãƒ¼ã‚¿æ§‹é€ ** ã®è¦³ç‚¹ã‹ã‚‰ç†è§£ã—ã¦ã„ãã¾ã™ã€‚å®Ÿéš›ã«ã‚³ãƒ¼ãƒ‰ã‚’å‹•ã‹ã—ãªãŒã‚‰ã€ãƒ†ã‚­ã‚¹ãƒˆãŒã©ã®ã‚ˆã†ã«å‡¦ç†ã•ã‚Œã¦ã€Œæ¬¡ã®å˜èªã€ã‚’äºˆæ¸¬ã™ã‚‹ã®ã‹ã‚’è¿½ã„ã‹ã‘ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

# Transformer ã¨ã¯ï¼Ÿ

## Transformer ã®èª•ç”Ÿ

Transformer ã¯ã€2017 å¹´ã« Google ãŒç™ºè¡¨ã—ãŸè«–æ–‡ã€Œ[Attention Is All You Need](https://arxiv.org/abs/1706.03762)ã€ã§ææ¡ˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚ãã‚Œã¾ã§ã® RNNï¼ˆå†å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ã«ä»£ã‚ã‚Šã€**Attentionï¼ˆæ³¨æ„æ©Ÿæ§‹ï¼‰** ã‚’ä¸­å¿ƒã«æ®ãˆãŸè¨­è¨ˆãŒç‰¹å¾´ã§ã™ã€‚

## GPT-2 ã¨ã¯ï¼Ÿ

GPT-2ï¼ˆGenerative Pre-trained Transformer 2ï¼‰ã¯ã€OpenAI ãŒ 2019 å¹´ã«å…¬é–‹ã—ãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ã•ã‚Œã¦ãŠã‚Šã€**æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹** ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ã¦ã„ã¾ã™ã€‚

# ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

## å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª

```python
import torch
from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel
import matplotlib.pyplot as plt
import numpy as np
```

ä¸»ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼š

- **transformers**: Hugging Face ãŒæä¾›ã™ã‚‹ Transformer ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- **torch**: PyTorchï¼ˆæ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼‰
- **matplotlib**: ã‚°ãƒ©ãƒ•æç”»ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

## GPT-2 ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰

```python
# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2', attn_implementation="eager")
model_lm = GPT2LMHeadModel.from_pretrained('gpt2', attn_implementation="eager")

# æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®šï¼ˆå­¦ç¿’ã¯ã—ãªã„ï¼‰
model.eval()
model_lm.eval()
```

ã“ã“ã§ã¯ 2 ç¨®é¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™ï¼š

- **GPT2Model**: Transformer ã®åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ï¼ˆéš ã‚ŒçŠ¶æ…‹ã‚’å‡ºåŠ›ï¼‰
- **GPT2LMHeadModel**: è¨€èªãƒ¢ãƒ‡ãƒ«ç”¨ï¼ˆæ¬¡ã®å˜èªã®ç¢ºç‡ã‚’å‡ºåŠ›ï¼‰

## GPT-2 ã®åŸºæœ¬ã‚¹ãƒšãƒƒã‚¯

```python
config = model.config
print(f"èªå½™ã‚µã‚¤ã‚º: {config.vocab_size}")
print(f"éš ã‚Œå±¤ã®æ¬¡å…ƒ: {config.n_embd}")
print(f"Transformer ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {config.n_layer}")
print(f"Attention ãƒ˜ãƒƒãƒ‰æ•°: {config.n_head}")
print(f"æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {config.n_positions}")
```

| é …ç›®         | å€¤    | èª¬æ˜                                                      |
| ------------ | ----- | --------------------------------------------------------- |
| vocab_size   | 50257 | èªå½™æ•°ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒçŸ¥ã£ã¦ã„ã‚‹ã€Œå˜èªã€ã®æ•°ï¼‰                  |
| hidden_size  | 768   | å†…éƒ¨è¡¨ç¾ã®æ¬¡å…ƒæ•°ï¼ˆå„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ 768 æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ç¾ï¼‰ |
| n_layer      | 12    | Transformer ãƒ–ãƒ­ãƒƒã‚¯ã®å±¤æ•°                                |
| n_head       | 12    | Attention ãƒ˜ãƒƒãƒ‰ã®æ•°ï¼ˆç•°ãªã‚‹è¦³ç‚¹ã§æ–‡è„ˆã‚’æ‰ãˆã‚‹ï¼‰          |
| max_position | 1024  | ä¸€åº¦ã«å‡¦ç†ã§ãã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°                            |

# Tokenizer: ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å­—ã«å¤‰æ›ã™ã‚‹

## ãªãœæ•°å­—ã«å¤‰æ›ã™ã‚‹ã®ã‹ï¼Ÿ

ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯æ–‡å­—åˆ—ã‚’ãã®ã¾ã¾ç†è§£ã§ãã¾ã›ã‚“ã€‚æ•°å­¦çš„ãªæ¼”ç®—ã‚’è¡Œã†ãŸã‚ã«ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ **æ•°å­—ã®é…åˆ—** ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®å¤‰æ›ã‚’è¡Œã†ã®ãŒ **Tokenizerï¼ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼‰** ã§ã™ã€‚

## ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®æµã‚Œ

ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ã¾ã§ã®æµã‚Œã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

```
ãƒ†ã‚­ã‚¹ãƒˆ â†’ ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆéƒ¨åˆ†æ–‡å­—åˆ—ï¼‰ â†’ Token IDï¼ˆæ•°å­—ï¼‰
```

å®Ÿéš›ã«è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

```python
text = "Hello, how are you?"

# ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²
tokens = tokenizer.tokenize(text)
print(f"å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ: {text}")
print(f"ãƒˆãƒ¼ã‚¯ãƒ³åˆ—: {tokens}")
print(f"ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(tokens)}")
```

å‡ºåŠ›ï¼š

```
å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ: Hello, how are you?
ãƒˆãƒ¼ã‚¯ãƒ³åˆ—: ['Hello', ',', ' how', ' are', ' you', '?']
ãƒˆãƒ¼ã‚¯ãƒ³æ•°: 6
```

GPT-2 ã¯ **BPEï¼ˆByte Pair Encodingï¼‰** ã¨ã„ã†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚’è¡Œã„ã¾ã™ã€‚å˜èªå˜ä½ã§ã¯ãªãã€ã‚ˆãå‡ºç¾ã™ã‚‹æ–‡å­—åˆ—ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¦åˆ†å‰²ã™ã‚‹ãŸã‚ã€ã€Œhowã€ãŒã€Œ howã€ï¼ˆå…ˆé ­ã«ã‚¹ãƒšãƒ¼ã‚¹ä»˜ãï¼‰ã®ã‚ˆã†ã«ãªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

## Token ID ã¸ã®å¤‰æ›

```python
# ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ Token ID ã«å¤‰æ›
token_ids = tokenizer.encode(text)
print(f"Token IDs: {token_ids}")

# å„ãƒˆãƒ¼ã‚¯ãƒ³ã¨ ID ã®å¯¾å¿œ
for token, token_id in zip(tokens, token_ids):
    print(f"  '{token}' -> {token_id}")
```

å‡ºåŠ›ï¼š

```
Token IDs: [15496, 11, 703, 389, 345, 30]
  'Hello' -> 15496
  ',' -> 11
  ' how' -> 703
  ' are' -> 389
  ' you' -> 345
  '?' -> 30
```

å„ãƒˆãƒ¼ã‚¯ãƒ³ã«ã¯ä¸€æ„ã® IDï¼ˆ0 ã€œ 50256 ã®æ•´æ•°ï¼‰ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

## ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ç”¨ã®å½¢å¼

å®Ÿéš›ã«ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã™ã‚‹éš›ã¯ã€`tokenizer()` ã‚’ç›´æ¥å‘¼ã³å‡ºã™ã®ãŒä¾¿åˆ©ã§ã™ï¼š

```python
inputs = tokenizer(text, return_tensors='pt')

print(f"input_ids: {inputs['input_ids']}")
print(f"input_ids.shape: {inputs['input_ids'].shape}")  # (batch_size, sequence_length)
print(f"attention_mask: {inputs['attention_mask']}")
```

å‡ºåŠ›ï¼š

```
input_ids: tensor([[15496,    11,   703,   389,   345,    30]])
input_ids.shape: torch.Size([1, 6])  # (ãƒãƒƒãƒã‚µã‚¤ã‚º=1, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·=6)
attention_mask: tensor([[1, 1, 1, 1, 1, 1]])
```

- **input_ids**: Token ID ã®ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆ2 æ¬¡å…ƒé…åˆ—ï¼‰
- **attention_mask**: å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ Attention è¨ˆç®—ã«å«ã‚ã‚‹ã‹ã©ã†ã‹ï¼ˆ1 = å«ã‚ã‚‹ï¼‰

# Embedding: æ•°å­—ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹

## ãªãœãƒ™ã‚¯ãƒˆãƒ«ã«ã™ã‚‹ã®ã‹ï¼Ÿ

Token ID ã¯å˜ãªã‚‹æ•´æ•°ã§ã€æ•°å­—è‡ªä½“ã«ã¯æ„å‘³ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ä¾‹ãˆã°ã€ã€Œcatã€ãŒ 100 ã§ã€Œdogã€ãŒ 200 ã ã¨ã—ã¦ã‚‚ã€200 ãŒ 100 ã‚ˆã‚Šã€Œå¤§ãã„ã€ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

ãã“ã§ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ **é«˜æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆ768 æ¬¡å…ƒï¼‰** ã«å¤‰æ›ã—ã¾ã™ã€‚ã“ã®ãƒ™ã‚¯ãƒˆãƒ«ã¯ã€Œå˜èªã®æ„å‘³ã€ã‚’æ•°å­¦çš„ã«è¡¨ç¾ã—ã¦ãŠã‚Šã€ä¼¼ãŸæ„å‘³ã®å˜èªã¯è¿‘ã„ãƒ™ã‚¯ãƒˆãƒ«ã«ãªã‚‹ã‚ˆã†å­¦ç¿’ã•ã‚Œã¦ã„ã¾ã™ã€‚

## ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ã¨ä½ç½®åŸ‹ã‚è¾¼ã¿

GPT-2 ã§ã¯ 2 ç¨®é¡ã®åŸ‹ã‚è¾¼ã¿ï¼ˆEmbeddingï¼‰ã‚’ä½¿ã„ã¾ã™ï¼š

```python
# ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿å±¤
print(f"ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ (wte): {model.wte.weight.shape}")  # (50257, 768)

# ä½ç½®åŸ‹ã‚è¾¼ã¿å±¤
print(f"ä½ç½®åŸ‹ã‚è¾¼ã¿ (wpe): {model.wpe.weight.shape}")  # (1024, 768)
```

| åŸ‹ã‚è¾¼ã¿                      | å½¢çŠ¶         | å½¹å‰²                                   |
| ----------------------------- | ------------ | -------------------------------------- |
| wte (Word Token Embedding)    | (50257, 768) | å„ãƒˆãƒ¼ã‚¯ãƒ³ã®ã€Œæ„å‘³ã€ã‚’è¡¨ã™ãƒ™ã‚¯ãƒˆãƒ«     |
| wpe (Word Position Embedding) | (1024, 768)  | å„ä½ç½®ã®ã€Œæ–‡ä¸­ã§ã®å ´æ‰€ã€ã‚’è¡¨ã™ãƒ™ã‚¯ãƒˆãƒ« |

## åŸ‹ã‚è¾¼ã¿ã®è¨ˆç®—

æœ€çµ‚çš„ãªå…¥åŠ›ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ã¨ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’ **è¶³ã—åˆã‚ã›ã¦** ä½œã‚Šã¾ã™ï¼š

```python
text = "Hello world"
inputs = tokenizer(text, return_tensors='pt')
input_ids = inputs['input_ids']

# ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
token_embeddings = model.wte(input_ids)
print(f"ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿: {token_embeddings.shape}")  # (1, 2, 768)

# ä½ç½® ID ã‚’ä½œæˆ
seq_length = input_ids.shape[1]
position_ids = torch.arange(seq_length).unsqueeze(0)  # [[0, 1]]

# ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
position_embeddings = model.wpe(position_ids)
print(f"ä½ç½®åŸ‹ã‚è¾¼ã¿: {position_embeddings.shape}")  # (1, 2, 768)

# æœ€çµ‚çš„ãªå…¥åŠ› = ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ + ä½ç½®åŸ‹ã‚è¾¼ã¿
final_embeddings = token_embeddings + position_embeddings
print(f"æœ€çµ‚å…¥åŠ›: {final_embeddings.shape}")  # (1, 2, 768)
```

ã“ã® `final_embeddings` ãŒ Transformer ãƒ–ãƒ­ãƒƒã‚¯ã¸ã®å…¥åŠ›ã¨ãªã‚Šã¾ã™ã€‚

# ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›: æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹

## åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›

ã¾ãšã¯åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ï¼ˆGPT2Modelï¼‰ã®å‡ºåŠ›ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ï¼š

```python
text = "The quick brown fox"
inputs = tokenizer(text, return_tensors='pt')

with torch.no_grad():  # å‹¾é…è¨ˆç®—ã‚’ã‚ªãƒ•ï¼ˆæ¨è«–æ™‚ï¼‰
    outputs = model(**inputs)

print(f"å…¥åŠ›å½¢çŠ¶: {inputs['input_ids'].shape}")  # (1, 4)
print(f"å‡ºåŠ›å½¢çŠ¶: {outputs.last_hidden_state.shape}")  # (1, 4, 768)
```

å‡ºåŠ›ï¼š

```
å…¥åŠ›å½¢çŠ¶: torch.Size([1, 4])  # (ãƒãƒƒãƒã‚µã‚¤ã‚º=1, ãƒˆãƒ¼ã‚¯ãƒ³æ•°=4)
å‡ºåŠ›å½¢çŠ¶: torch.Size([1, 4, 768])  # (ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹, éš ã‚Œæ¬¡å…ƒ)
```

**last_hidden_state** ã¯ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã® **æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸè¡¨ç¾** ã§ã™ã€‚å…¥åŠ›æ™‚ã®åŸ‹ã‚è¾¼ã¿ã¯å˜èªå˜ç‹¬ã®æ„å‘³ã§ã—ãŸãŒã€Transformer ã‚’é€šéã™ã‚‹ã“ã¨ã§å‘¨å›²ã®å˜èªã®æƒ…å ±ãŒåæ˜ ã•ã‚Œã¾ã™ã€‚

## è¨€èªãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ï¼ˆLogitsï¼‰

æ¬¡ã«ã€è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆGPT2LMHeadModelï¼‰ã®å‡ºåŠ›ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

```python
with torch.no_grad():
    outputs_lm = model_lm(**inputs)

print(f"logits å½¢çŠ¶: {outputs_lm.logits.shape}")  # (1, 4, 50257)
```

å‡ºåŠ›ï¼š

```
logits å½¢çŠ¶: torch.Size([1, 4, 50257])  # (ãƒãƒƒãƒ, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹, èªå½™ã‚µã‚¤ã‚º)
```

**logits** ã¯ã€å„ä½ç½®ã§ã®ã€Œæ¬¡ã«æ¥ã‚‹å˜èªã®ç¢ºç‡ï¼ˆæœªæ­£è¦åŒ–ï¼‰ã€ã‚’è¡¨ã—ã¾ã™ã€‚æœ€å¾Œã®æ¬¡å…ƒãŒ 50257 ãªã®ã¯ã€èªå½™ã«ã‚ã‚‹å…¨å˜èªãã‚Œãã‚Œã®ã‚¹ã‚³ã‚¢ã‚’æŒã£ã¦ã„ã‚‹ãŸã‚ã§ã™ã€‚

## æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã—ã¦ã¿ã‚‹

ã€ŒThe quick brown foxã€ã®ç¶šãã‚’äºˆæ¸¬ã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

```python
# æœ€å¾Œã®ä½ç½®ã® logits ã‚’å–å¾—
last_logits = outputs_lm.logits[0, -1, :]  # (50257,)

# softmax ã§ç¢ºç‡ã«å¤‰æ›
probs = torch.softmax(last_logits, dim=-1)

# ä¸Šä½ 5 ä»¶ã‚’è¡¨ç¤º
top_k = 5
top_probs, top_ids = torch.topk(probs, top_k)

print(f"'{text}' ã®æ¬¡ã«æ¥ã‚‹å¯èƒ½æ€§ãŒé«˜ã„å˜èª:")
for prob, token_id in zip(top_probs, top_ids):
    token = tokenizer.decode([token_id])
    print(f"  '{token}': {prob.item():.4f}")
```

å‡ºåŠ›ä¾‹ï¼š

```
'The quick brown fox' ã®æ¬¡ã«æ¥ã‚‹å¯èƒ½æ€§ãŒé«˜ã„å˜èª:
  ' jumps': 0.1842
  ' jumped': 0.0821
  ',': 0.0534
  ' is': 0.0312
  ' runs': 0.0287
```

ã€ŒThe quick brown fox **jumps** over the lazy dogã€ã¨ã„ã†æœ‰åãªãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ãŸã‚ã€ã€Œjumpsã€ãŒæœ€ã‚‚ç¢ºç‡ãŒé«˜ããªã£ã¦ã„ã¾ã™ã€‚

# Attention: ã©ã“ã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹

## Attention ã¨ã¯ï¼Ÿ

Attentionï¼ˆæ³¨æ„æ©Ÿæ§‹ï¼‰ã¯ã€Transformer ã®æ ¸ã¨ãªã‚‹ä»•çµ„ã¿ã§ã™ã€‚æ–‡ä¸­ã®å„å˜èªãŒã€**ä»–ã®ã©ã®å˜èªã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹** ã‚’è¨ˆç®—ã—ã¾ã™ã€‚

ä¾‹ãˆã°ã€ã€ŒThe cat sat on the matã€ã¨ã„ã†æ–‡ã§ã€Œsatã€ã¨ã„ã†å˜èªã‚’å‡¦ç†ã™ã‚‹ã¨ãã€Attention ã«ã‚ˆã£ã¦ã€Œcatã€ã«å¼·ãæ³¨ç›®ã™ã‚‹ã“ã¨ã§ã€ã€Œèª°ãŒåº§ã£ãŸã®ã‹ã€ã¨ã„ã†æƒ…å ±ã‚’å–ã‚Šè¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚

## Attention Weights ã®å–å¾—

Attention ã®é‡ã¿ã‚’å¯è¦–åŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

```python
text = "The cat sat on the mat"
inputs = tokenizer(text, return_tensors='pt')

with torch.no_grad():
    outputs = model(**inputs, output_attentions=True)

# Attention weights ã‚’å–å¾—
attentions = outputs.attentions
print(f"ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {len(attentions)}")
print(f"å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å½¢çŠ¶: {attentions[0].shape}")
```

å‡ºåŠ›ï¼š

```
ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: 12
å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å½¢çŠ¶: torch.Size([1, 12, 7, 7])  # (ãƒãƒƒãƒ, ãƒ˜ãƒƒãƒ‰æ•°, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹, ã‚·ãƒ¼ã‚±ãƒ³ã‚¹)
```

Attention ã®å½¢çŠ¶ `(1, 12, 7, 7)` ã®æ„å‘³ï¼š

- **1**: ãƒãƒƒãƒã‚µã‚¤ã‚º
- **12**: Attention ãƒ˜ãƒƒãƒ‰ã®æ•°ï¼ˆç•°ãªã‚‹è¦³ç‚¹ã§æ³¨ç›®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ï¼‰
- **7 Ã— 7**: 7 ãƒˆãƒ¼ã‚¯ãƒ³åŒå£«ã®æ³¨ç›®åº¦åˆã„ï¼ˆè¡Œ = Queryã€åˆ— = Keyï¼‰

## ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–

![](/images/transformer-io/attention-layer0-head0.png)

ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®è¦‹æ–¹ï¼š

- **æ¨ªè»¸ï¼ˆKeyï¼‰**: æ³¨ç›®ã•ã‚Œã‚‹å´ã®ãƒˆãƒ¼ã‚¯ãƒ³
- **ç¸¦è»¸ï¼ˆQueryï¼‰**: æ³¨ç›®ã™ã‚‹å´ã®ãƒˆãƒ¼ã‚¯ãƒ³
- **è‰²ã®æ¿ƒã•**: æ³¨ç›®åº¦åˆã„ï¼ˆæ¿ƒã„ã»ã©å¼·ãæ³¨ç›®ï¼‰

## Causal Attentionï¼ˆå› æœçš„æ³¨æ„ï¼‰

GPT-2 ã¯ **è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«** ãªã®ã§ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã¯ **è‡ªåˆ†ã‚ˆã‚Šå‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ã—ã‹æ³¨ç›®ã§ãã¾ã›ã‚“**ã€‚ã“ã‚Œã‚’ Causal Attentionï¼ˆã¾ãŸã¯ Masked Self-Attentionï¼‰ã¨å‘¼ã³ã¾ã™ã€‚

ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’è¦‹ã‚‹ã¨ã€å¯¾è§’ç·šã‚ˆã‚Šä¸Šï¼ˆæœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã®å€¤ãŒã™ã¹ã¦ 0 ã«ãªã£ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚

```python
attn = attentions[0][0, 0].numpy()  # Layer 0, Head 0
print("Attention è¡Œåˆ—ï¼ˆä¸‹ä¸‰è§’ã®ã¿æœ‰åŠ¹ï¼‰:")
print(np.round(attn, 2))
```

ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯ã€Œã‚«ãƒ³ãƒ‹ãƒ³ã‚°ã€ã›ãšã«ã€éå»ã®æƒ…å ±ã®ã¿ã‹ã‚‰æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

## è¤‡æ•°ãƒ˜ãƒƒãƒ‰ã® Attention ãƒ‘ã‚¿ãƒ¼ãƒ³

GPT-2 ã«ã¯ 12 å€‹ã® Attention ãƒ˜ãƒƒãƒ‰ãŒã‚ã‚Šã€ãã‚Œãã‚Œç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ³¨ç›®ã—ã¾ã™ï¼š

![](/images/transformer-io/attention-multi-heads.png)

å„ãƒ˜ãƒƒãƒ‰ã®å½¹å‰²ï¼ˆä¾‹ï¼‰ï¼š

- **ç›´å‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«æ³¨ç›®**: å±€æ‰€çš„ãªæ–‡æ³•é–¢ä¿‚ã‚’æ‰ãˆã‚‹
- **æ–‡é ­ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«æ³¨ç›®**: æ–‡å…¨ä½“ã®ä¸»é¡Œã‚’æ‰ãˆã‚‹
- **ç‰¹å®šã®å“è©ã«æ³¨ç›®**: å‹•è©ã¨ä¸»èªã®é–¢ä¿‚ã‚’æ‰ãˆã‚‹

è¤‡æ•°ã®ãƒ˜ãƒƒãƒ‰ã‚’æŒã¤ã“ã¨ã§ã€å¤šæ§˜ãªè¦³ç‚¹ã‹ã‚‰æ–‡è„ˆã‚’ç†è§£ã§ãã¾ã™ã€‚

# å…¥åŠ›é•·ã¨ãƒãƒƒãƒå‡¦ç†

## å…¥åŠ›é•·ã«ã‚ˆã‚‹å‡ºåŠ›å½¢çŠ¶ã®å¤‰åŒ–

Transformer ã®å‡ºåŠ›ã¯ã€å…¥åŠ›ã®é•·ã•ã«å¿œã˜ã¦å¤‰ã‚ã‚Šã¾ã™ï¼š

```python
short_text = "Hi"
long_text = "The quick brown fox jumps over the lazy dog"

inputs_short = tokenizer(short_text, return_tensors='pt')
inputs_long = tokenizer(long_text, return_tensors='pt')

print(f"çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆ: ãƒˆãƒ¼ã‚¯ãƒ³æ•° = {inputs_short['input_ids'].shape[1]}")
print(f"é•·ã„ãƒ†ã‚­ã‚¹ãƒˆ: ãƒˆãƒ¼ã‚¯ãƒ³æ•° = {inputs_long['input_ids'].shape[1]}")

with torch.no_grad():
    outputs_short = model_lm(**inputs_short)
    outputs_long = model_lm(**inputs_long)

print(f"çŸ­ã„å‡ºåŠ›: {outputs_short.logits.shape}")  # (1, 1, 50257)
print(f"é•·ã„å‡ºåŠ›: {outputs_long.logits.shape}")   # (1, 10, 50257)
```

## æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·

GPT-2 ã¯æœ€å¤§ 1024 ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§ã—ã‹å‡¦ç†ã§ãã¾ã›ã‚“ã€‚ãã‚Œã‚’è¶…ãˆã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ï¼ˆtruncationï¼‰ãŒå¿…è¦ã§ã™ï¼š

```python
very_long_text = "Hello " * 600  # éå¸¸ã«é•·ã„ãƒ†ã‚­ã‚¹ãƒˆ

inputs = tokenizer(
    very_long_text,
    return_tensors='pt',
    truncation=True,  # åˆ‡ã‚Šè©°ã‚ã‚’æœ‰åŠ¹åŒ–
    max_length=1024   # æœ€å¤§é•·ã‚’æŒ‡å®š
)
print(f"åˆ‡ã‚Šè©°ã‚å¾Œ: {inputs['input_ids'].shape[1]} ãƒˆãƒ¼ã‚¯ãƒ³")
```

## ãƒãƒƒãƒå‡¦ç†ã¨ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°

è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒæ™‚ã«å‡¦ç†ã™ã‚‹å ´åˆã€é•·ã•ã‚’æƒãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š

```python
# GPT-2 ã«ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ pad_token ãŒãªã„ã®ã§è¨­å®š
tokenizer.pad_token = tokenizer.eos_token

texts = [
    "Hello",
    "Hello, how are you?",
    "The quick brown fox jumps over the lazy dog"
]

# ãƒãƒƒãƒå‡¦ç†ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚ã‚Šï¼‰
batch_inputs = tokenizer(texts, return_tensors='pt', padding=True)

print(f"ãƒãƒƒãƒå½¢çŠ¶: {batch_inputs['input_ids'].shape}")  # (3, 10)
print(f"Attention Mask:\n{batch_inputs['attention_mask']}")
```

å‡ºåŠ›ï¼š

```
ãƒãƒƒãƒå½¢çŠ¶: torch.Size([3, 10])  # 3 ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã€æœ€å¤§ 10 ãƒˆãƒ¼ã‚¯ãƒ³
Attention Mask:
tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # å®Ÿéš›ã¯ 1 ãƒˆãƒ¼ã‚¯ãƒ³ã€æ®‹ã‚Šãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],  # 6 ãƒˆãƒ¼ã‚¯ãƒ³
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) # 10 ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæœ€é•·ï¼‰
```

**attention_mask** ã®å½¹å‰²ï¼š

- **1**: å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆAttention è¨ˆç®—ã«å«ã‚ã‚‹ï¼‰
- **0**: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç„¡è¦–ã™ã‚‹ï¼‰

# ã¾ã¨ã‚: ãƒ‡ãƒ¼ã‚¿ã®æµã‚Œå…¨ä½“åƒ

ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¬¡ã®å˜èªäºˆæ¸¬ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã®æµã‚Œã‚’æ•´ç†ã—ã¾ã—ã‚‡ã†ï¼š

```
ãƒ†ã‚­ã‚¹ãƒˆ
   â†“ Tokenizer
Token ID: (Batch, Sequence)
   â†“ Embedding
åŸ‹ã‚è¾¼ã¿: (Batch, Sequence, Hidden=768)
   â†“ Transformer Ã— 12 å±¤
Hidden State: (Batch, Sequence, Hidden=768)
   â†“ LM Head
Logits: (Batch, Sequence, Vocab=50257)
   â†“ Softmax
ç¢ºç‡åˆ†å¸ƒ â†’ æ¬¡ã®å˜èªã‚’äºˆæ¸¬
```

## ä¸»è¦ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¾ã¨ã‚

| ãƒ‡ãƒ¼ã‚¿              | å½¢çŠ¶                            | èª¬æ˜                       |
| ------------------- | ------------------------------- | -------------------------- |
| input_ids           | (Batch, Sequence)               | ãƒˆãƒ¼ã‚¯ãƒ³ã® ID åˆ—           |
| attention_mask      | (Batch, Sequence)               | ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒã‚¹ã‚¯         |
| token_embeddings    | (Batch, Sequence, 768)          | ãƒˆãƒ¼ã‚¯ãƒ³ã®æ„å‘³ãƒ™ã‚¯ãƒˆãƒ«     |
| position_embeddings | (Batch, Sequence, 768)          | ä½ç½®ã®ãƒ™ã‚¯ãƒˆãƒ«             |
| last_hidden_state   | (Batch, Sequence, 768)          | æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸè¡¨ç¾         |
| logits              | (Batch, Sequence, 50257)        | æ¬¡ã®å˜èªã®ç¢ºç‡ï¼ˆæœªæ­£è¦åŒ–ï¼‰ |
| attentions          | (Batch, 12, Sequence, Sequence) | Attention ã®é‡ã¿           |

# å‚è€ƒãƒªãƒ³ã‚¯

- [Hugging Face Transformers ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://huggingface.co/docs/transformers)
- [Attention Is All You Needï¼ˆåŸè«–æ–‡ï¼‰](https://arxiv.org/abs/1706.03762)
- [GPT-2 è«–æ–‡ï¼ˆLanguage Models are Unsupervised Multitask Learnersï¼‰](https://openai.com/research/better-language-models)
- [The Illustrated Transformerï¼ˆå›³è§£ Transformerï¼‰](https://jalammar.github.io/illustrated-transformer/)
