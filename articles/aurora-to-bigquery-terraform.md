---
title: "Terraform ã§æ§‹ç¯‰ã™ã‚‹ Aurora â†’ BigQuery ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"
emoji: "ğŸ“Š"
type: "tech"
topics: ["terraform", "aws", "gcp", "bigquery", "dms"]
published: true
---

# ã¯ã˜ã‚ã«

RDB ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æåŸºç›¤ã§æ´»ç”¨ã—ãŸã„ã‚±ãƒ¼ã‚¹ã¯å¤šãã‚ã‚Šã¾ã™ã€‚ã—ã‹ã—ã€æœ¬ç•ª DB ã«ç›´æ¥ã‚¯ã‚¨ãƒªã‚’æŠ•ã’ã‚‹ã¨è² è·ã‚„ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®å•é¡ŒãŒç™ºç”Ÿã™ã‚‹ãŸã‚ã€åˆ†æç”¨é€”ã«ç‰¹åŒ–ã—ãŸãƒ‡ãƒ¼ã‚¿åŸºç›¤ã¸ãƒ‡ãƒ¼ã‚¿ã‚’é€£æºã™ã‚‹ã“ã¨ãŒä¸€èˆ¬çš„ã§ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€AWS Auroraï¼ˆMySQLï¼‰ã‹ã‚‰ Google BigQuery ã¸ãƒ‡ãƒ¼ã‚¿ã‚’é€£æºã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ Terraform ã§æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã‚’è§£èª¬ã—ã¾ã™ã€‚

# ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

![](/images/aurora-bigquery-pipeline.drawio.png)

æœ¬æ§‹æˆã§ã¯ä»¥ä¸‹ã®ãƒ•ãƒ­ãƒ¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’é€£æºã—ã¾ã™ï¼š

1. **AWS DMS Serverless** ãŒ Auroraï¼ˆReader Endpointï¼‰ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
2. **S3** ã« Parquet å½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
3. **Google Storage Transfer Service** ãŒ S3 ã‹ã‚‰ GCS ã¸ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€
4. **BigQuery External Table** ãŒ GCS ä¸Šã® Parquet ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥å‚ç…§

å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å½¹å‰²ï¼š

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ           | å½¹å‰²                                           |
| ------------------------ | ---------------------------------------------- |
| DMS Serverless           | Aurora â†’ S3 ã¸ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»å¤‰æ›ï¼ˆParquet åŒ–ï¼‰ |
| S3                       | ä¸€æ™‚çš„ãªãƒ‡ãƒ¼ã‚¿ä¿å­˜å ´æ‰€ï¼ˆAWS å´ï¼‰               |
| EventBridge Scheduler    | DMS ã®å®šæœŸå®Ÿè¡Œãƒˆãƒªã‚¬ãƒ¼                         |
| Storage Transfer Service | S3 â†’ GCS ã®ã‚¯ãƒ­ã‚¹ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ‡ãƒ¼ã‚¿è»¢é€            |
| GCS                      | åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜å ´æ‰€ï¼ˆGCP å´ï¼‰               |
| BigQuery External Table  | GCS ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥ã‚¯ã‚¨ãƒª                     |

# Aurora â†’ BigQuery é€£æºæ–¹æ³•ã®æ¯”è¼ƒ

Aurora ã‹ã‚‰ BigQuery ã¸ãƒ‡ãƒ¼ã‚¿ã‚’é€£æºã™ã‚‹ä»£è¡¨çš„ãªæ–¹æ³•ã¨ã—ã¦ã€GCP ãƒã‚¤ãƒ†ã‚£ãƒ–ã® **Datastream** ãŒã‚ã‚Šã¾ã™ã€‚æœ¬æ§‹æˆã¨æ¯”è¼ƒã—ã¦èª¬æ˜ã—ã¾ã™ã€‚

## Datastream + BigQuery ã¨ã®æ¯”è¼ƒ

| é …ç›®               | æœ¬æ§‹æˆï¼ˆDMS + Storage Transferï¼‰  | Datastream + BigQuery            |
| ------------------ | --------------------------------- | -------------------------------- |
| **Aurora å¯¾å¿œ**    | â—‹ï¼ˆDMS ãŒ Aurora ã‚’ç›´æ¥ã‚µãƒãƒ¼ãƒˆï¼‰ | â—‹ï¼ˆAurora MySQL å¯¾å¿œï¼‰           |
| **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**   | ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆçµŒç”±ï¼ˆS3/GCSï¼‰      | VPN / Cloud Interconnect å¿…è¦    |
| **ãƒ‡ãƒ¼ã‚¿å½¢å¼**     | Parquetï¼ˆã‚«ãƒ©ãƒ å‹ã€åœ§ç¸®åŠ¹ç‡é«˜ï¼‰   | BigQuery ãƒã‚¤ãƒ†ã‚£ãƒ–              |
| **CDC**            | â—‹ï¼ˆDMS CDC å¯¾å¿œï¼‰                 | â—‹ï¼ˆãƒã‚¤ãƒ†ã‚£ãƒ– CDCï¼‰              |
| **ãƒã‚¹ã‚­ãƒ³ã‚°**     | DMS ã§ã‚«ãƒ©ãƒ é™¤å¤– or BigQuery      | BigQuery ã®ã¿                    |
| **ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹**   | â—‹                                 | â—‹                                |
| **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§** | ä¸­ï¼ˆå®šæœŸãƒãƒƒãƒï¼‰                  | é«˜ï¼ˆç¶™ç¶šçš„ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰     |
| **ä¸­é–“ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸** | S3 / GCSï¼ˆParquet ä¿å­˜ï¼‰          | ä¸è¦ï¼ˆç›´æ¥ BigQuery ã¸ï¼‰         |

## æœ¬æ§‹æˆã‚’é¸ã‚“ã ç†ç”±

1. **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹æˆãŒã‚·ãƒ³ãƒ—ãƒ«**: VPN / Interconnect ä¸è¦ã§ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆçµŒç”±ã®è»¢é€ãŒå¯èƒ½
2. **Parquet å½¢å¼ã§ã®ä¸­é–“ä¿å­˜**: ã‚«ãƒ©ãƒ å‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§åœ§ç¸®åŠ¹ç‡ãŒé«˜ãã€BigQuery ä»¥å¤–ã®ãƒ„ãƒ¼ãƒ«ã‹ã‚‰ã‚‚å‚ç…§å¯èƒ½
3. **ã‚»ã‚­ãƒ¥ã‚¢ãªã‚¯ãƒ­ã‚¹ã‚¯ãƒ©ã‚¦ãƒ‰èªè¨¼**: OpenID Connect ãƒ•ã‚§ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚¢ã‚¯ã‚»ã‚¹ã‚­ãƒ¼ä¸è¦
4. **ç–çµåˆ**: å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒç‹¬ç«‹ã—ã¦ãŠã‚Šã€éšœå®³ã®å½±éŸ¿ç¯„å›²ãŒé™å®šçš„
5. **AWS å´ã®åˆ¶å¾¡**: DMS ã®æŸ”è»Ÿãªãƒ†ãƒ¼ãƒ–ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚„ã‚«ãƒ©ãƒ é™¤å¤–ãŒåˆ©ç”¨å¯èƒ½

# Terraform å®Ÿè£…

æœ¬è¨˜äº‹ã§ã¯ä¸»è¦ãªãƒªã‚½ãƒ¼ã‚¹å®šç¾©ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚Provider è¨­å®šã‚„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãªã©åŸºæœ¬çš„ãªéƒ¨åˆ†ã¯çœç•¥ã—ã¦ã„ã¾ã™ã€‚

## DMSï¼ˆDatabase Migration Serviceï¼‰

DMS Serverless ã‚’ä½¿ç”¨ã—ã¦ Aurora ã‹ã‚‰ S3 ã¸ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã™ã€‚

```hcl:dms.tf
# DMS ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ã‚µãƒ–ãƒãƒƒãƒˆã‚°ãƒ«ãƒ¼ãƒ—
resource "aws_dms_replication_subnet_group" "main" {
  replication_subnet_group_description = "DMS subnet group for analytics pipeline"
  replication_subnet_group_id          = "sample-analytics-dms-subnet-group"
  subnet_ids                           = local.private_subnet_ids
}

# Aurora ã‚’ã‚½ãƒ¼ã‚¹ã¨ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
resource "aws_dms_endpoint" "aurora_source" {
  endpoint_id   = "sample-aurora-source"
  endpoint_type = "source"
  engine_name   = "aurora"
  database_name = "myapp"
  username      = local.db_secrets.MYSQL_USER
  password      = local.db_secrets.MYSQL_PASSWORD
  server_name   = data.aws_rds_cluster.aurora_cluster.reader_endpoint
  port          = 3306
  ssl_mode      = "none"
}

# S3 ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆParquet å½¢å¼ï¼‰
resource "aws_dms_endpoint" "s3_target" {
  endpoint_id   = "sample-s3-target"
  endpoint_type = "target"
  engine_name   = "s3"

  s3_settings {
    service_access_role_arn          = aws_iam_role.dms_s3_target_role.arn
    bucket_name                      = aws_s3_bucket.analytics_data.id
    bucket_folder                    = ""
    compression_type                 = "GZIP"
    data_format                      = "parquet"
    encoding_type                    = "plain"
    dict_page_size_limit             = 1048576
    enable_statistics                = true
    include_op_for_full_load         = false
    parquet_timestamp_in_millisecond = true
    parquet_version                  = "parquet-2-0"
    row_group_length                 = 1024
    data_page_size                   = 1048576
    add_column_name                  = true
    timestamp_column_name            = "dms_timestamp"
    use_csv_no_sup_value             = false
    preserve_transactions            = false
    cdc_inserts_and_updates          = false
    cdc_inserts_only                 = false
  }
}

# DMS Serverless ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
resource "aws_dms_replication_config" "main" {
  replication_config_identifier = "sample-aurora-to-s3-serverless"
  replication_type              = "full-load"
  source_endpoint_arn           = aws_dms_endpoint.aurora_source.endpoint_arn
  target_endpoint_arn           = aws_dms_endpoint.s3_target.endpoint_arn
  start_replication             = false  # æ‰‹å‹•ã¾ãŸã¯ EventBridge ã‹ã‚‰å®Ÿè¡Œ

  # Serverless ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆè¨­å®š
  compute_config {
    replication_subnet_group_id = aws_dms_replication_subnet_group.main.id
    vpc_security_group_ids      = [local.dms_sg_id]
    min_capacity_units          = 1    # æœ€å° DCUï¼ˆ1 DCU = 2GB ãƒ¡ãƒ¢ãƒªï¼‰
    max_capacity_units          = 2    # æœ€å¤§ DCU
    multi_az                    = true # é«˜å¯ç”¨æ€§ã®ãŸã‚ãƒãƒ«ãƒ AZ
  }

  # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å¯¾è±¡ï¼‰
  table_mappings = jsonencode({
    rules = [
      {
        rule-type = "selection"
        rule-id   = "1"
        rule-name = "all-tables"
        object-locator = {
          schema-name = "myapp"
          table-name  = "%"
        }
        rule-action = "include"
      }
    ]
  })
}
```

### Parquet è¨­å®šã®ãƒã‚¤ãƒ³ãƒˆ

| è¨­å®šé …ç›®                | å€¤            | èª¬æ˜                                 |
| ----------------------- | ------------- | ------------------------------------ |
| `data_format`           | parquet       | Parquet å½¢å¼ã§å‡ºåŠ›                   |
| `compression_type`      | GZIP          | åœ§ç¸®ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’å‰Šæ¸›         |
| `parquet_version`       | parquet-2-0   | Parquet 2.0 å½¢å¼ï¼ˆã‚ˆã‚ŠåŠ¹ç‡çš„ï¼‰       |
| `timestamp_column_name` | dms_timestamp | DMS å®Ÿè¡Œæ™‚åˆ»ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ—ã‚’è¿½åŠ  |

## S3 ãƒã‚±ãƒƒãƒˆ

DMS ã®å‡ºåŠ›å…ˆã¨ãªã‚‹ S3 ãƒã‚±ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

```hcl:s3.tf
# DMS ã®ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ S3 ãƒã‚±ãƒƒãƒˆ
resource "aws_s3_bucket" "analytics_data" {
  bucket = "sample-analytics-data-myapp"
}

# ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°è¨­å®š
resource "aws_s3_bucket_versioning" "analytics_data" {
  bucket = aws_s3_bucket.analytics_data.id

  versioning_configuration {
    status = "Enabled"
  }
}

# æš—å·åŒ–è¨­å®š
resource "aws_s3_bucket_server_side_encryption_configuration" "analytics_data" {
  bucket = aws_s3_bucket.analytics_data.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}

# ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚¢ã‚¯ã‚»ã‚¹ãƒ–ãƒ­ãƒƒã‚¯
resource "aws_s3_bucket_public_access_block" "analytics_data" {
  bucket = aws_s3_bucket.analytics_data.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ãƒãƒªã‚·ãƒ¼ï¼ˆ7æ—¥å¾Œã«è‡ªå‹•å‰Šé™¤ï¼‰
resource "aws_s3_bucket_lifecycle_configuration" "analytics_data" {
  bucket = aws_s3_bucket.analytics_data.id

  rule {
    id     = "delete-old-data"
    status = "Enabled"

    filter {
      prefix = ""
    }

    expiration {
      days = 7
    }
  }
}
```

## EventBridge Scheduler

DMS ã‚’å®šæœŸå®Ÿè¡Œã™ã‚‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’è¨­å®šã—ã¾ã™ã€‚

```hcl:eventbridge.tf
# 3æ™‚é–“ã”ã¨ã« DMS ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
resource "aws_scheduler_schedule" "dms_task_schedule" {
  name        = "sample-dms-task-schedule"
  description = "Run DMS task every 3 hours"

  # 0, 3, 6, 9, 12, 15, 18, 21æ™‚ã«å®Ÿè¡Œ
  schedule_expression          = "cron(0 0-21/3 * * ? *)"
  schedule_expression_timezone = "Asia/Tokyo"

  flexible_time_window {
    mode = "OFF"
  }

  # DMS Serverless ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
  target {
    arn      = "arn:aws:scheduler:::aws-sdk:databasemigration:startReplication"
    role_arn = aws_iam_role.eventbridge_dms_role.arn

    input = jsonencode({
      ReplicationConfigArn = aws_dms_replication_config.main.arn
      StartReplicationType = "reload-target"  # ãƒ•ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚’å†å®Ÿè¡Œ
    })

    retry_policy {
      maximum_event_age_in_seconds = 3600  # 1æ™‚é–“
      maximum_retry_attempts       = 2
    }
  }

  state = "ENABLED"

  depends_on = [
    aws_iam_role_policy_attachment.eventbridge_dms
  ]
}
```

## IAM ãƒ­ãƒ¼ãƒ«ãƒ»ãƒãƒªã‚·ãƒ¼ï¼ˆAWSï¼‰

DMSã€EventBridgeã€GCP Storage Transfer Service ç”¨ã® IAM ã‚’è¨­å®šã—ã¾ã™ã€‚

```hcl:iam.tf
# ========================================
# DMS ç”¨ IAM ãƒ­ãƒ¼ãƒ«
# ========================================

# DMS ãŒ VPC ãƒªã‚½ãƒ¼ã‚¹ã‚’ç®¡ç†ã™ã‚‹ãŸã‚ã®ãƒ­ãƒ¼ãƒ«ï¼ˆå›ºå®šåç§°å¿…é ˆï¼‰
resource "aws_iam_role" "dms_access_role" {
  name = "dms-vpc-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "dms.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "dms_vpc_management" {
  role       = aws_iam_role.dms_access_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonDMSVPCManagementRole"
}

# DMS ãŒ CloudWatch ãƒ­ã‚°ã‚’æ›¸ãè¾¼ã‚€ãŸã‚ã®ãƒ­ãƒ¼ãƒ«
resource "aws_iam_role" "dms_cloudwatch_logs_role" {
  name = "dms-cloudwatch-logs-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "dms.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "dms_cloudwatch_logs" {
  role       = aws_iam_role.dms_cloudwatch_logs_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonDMSCloudWatchLogsRole"
}

# DMS ãŒ S3 ãƒã‚±ãƒƒãƒˆã¸æ›¸ãè¾¼ã‚€ãŸã‚ã®ãƒ­ãƒ¼ãƒ«
resource "aws_iam_role" "dms_s3_target_role" {
  name = "sample-dms-s3-target-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "dms.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_policy" "dms_s3_target_policy" {
  name = "sample-dms-s3-target-policy"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:ListBucket",
          "s3:GetBucketLocation"
        ]
        Resource = aws_s3_bucket.analytics_data.arn
      },
      {
        Effect = "Allow"
        Action = [
          "s3:PutObject",
          "s3:GetObject",
          "s3:DeleteObject",
          "s3:PutObjectAcl"
        ]
        Resource = "${aws_s3_bucket.analytics_data.arn}/*"
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "dms_s3_target" {
  role       = aws_iam_role.dms_s3_target_role.name
  policy_arn = aws_iam_policy.dms_s3_target_policy.arn
}

# ========================================
# EventBridge ç”¨ IAM ãƒ­ãƒ¼ãƒ«
# ========================================

resource "aws_iam_role" "eventbridge_dms_role" {
  name = "sample-eventbridge-dms-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Principal = {
          Service = [
            "scheduler.amazonaws.com",
            "events.amazonaws.com"
          ]
        }
        Action = "sts:AssumeRole"
      }
    ]
  })
}

resource "aws_iam_policy" "eventbridge_dms_policy" {
  name        = "sample-eventbridge-dms-policy"
  description = "Policy for EventBridge to start DMS Serverless replications"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "dms:StartReplication",
          "dms:DescribeReplications",
          "dms:StopReplication"
        ]
        Resource = aws_dms_replication_config.main.arn
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "eventbridge_dms" {
  role       = aws_iam_role.eventbridge_dms_role.name
  policy_arn = aws_iam_policy.eventbridge_dms_policy.arn
}

# ========================================
# GCP Storage Transfer Service ç”¨ IAM ãƒ­ãƒ¼ãƒ«
# ========================================

# GCP STS ãŒ S3 ã‚’èª­ã¿å–ã‚‹ãŸã‚ã®ãƒãƒªã‚·ãƒ¼
resource "aws_iam_policy" "s3_read_for_gcp_sts" {
  name        = "sample-S3ReadAccessForGCP-STS"
  description = "Allows read-only access to analytics S3 bucket for GCP STS."

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:GetBucketLocation",
          "s3:ListBucket"
        ]
        Resource = aws_s3_bucket.analytics_data.arn
      },
      {
        Effect   = "Allow"
        Action   = "s3:GetObject"
        Resource = "${aws_s3_bucket.analytics_data.arn}/*"
      },
    ]
  })
}

# GCP STS ãŒå¼•ãå—ã‘ã‚‹ IAM ãƒ­ãƒ¼ãƒ«ï¼ˆWeb Identity ãƒ•ã‚§ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
resource "aws_iam_role" "gcp_sts_role" {
  name = "sample-GCP-StorageTransferRole"

  # GCP STS ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰ã® AssumeRole ã‚’è¨±å¯
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Principal = {
          "Federated" = "accounts.google.com"
        }
        Action = "sts:AssumeRoleWithWebIdentity"
        Condition = {
          "StringEquals" = {
            "accounts.google.com:sub" = data.google_storage_transfer_project_service_account.default.subject_id
          }
        }
      },
    ]
  })
}

resource "aws_iam_role_policy_attachment" "attach_s3_read_policy" {
  role       = aws_iam_role.gcp_sts_role.name
  policy_arn = aws_iam_policy.s3_read_for_gcp_sts.arn
}
```

## GCS ãƒã‚±ãƒƒãƒˆãƒ»Storage Transfer Job

S3 ã‹ã‚‰ GCS ã¸ã®ãƒ‡ãƒ¼ã‚¿è»¢é€ã‚’è¨­å®šã—ã¾ã™ã€‚

```hcl:gcs.tf
# GCP Storage Transfer Service ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’å–å¾—
data "google_storage_transfer_project_service_account" "default" {
  project = var.gcp_project_id
}

# è»¢é€å…ˆã® GCS ãƒã‚±ãƒƒãƒˆ
resource "google_storage_bucket" "analytics_data" {
  name          = "sample-analytics-data-myapp"
  location      = var.gcp_region
  storage_class = "STANDARD"

  versioning {
    enabled = true
  }

  # 7æ—¥å¾Œã«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å‰Šé™¤
  lifecycle_rule {
    condition {
      age = 7
    }
    action {
      type = "Delete"
    }
  }

  public_access_prevention = "enforced"
}

# Storage Transfer Service ã«ãƒã‚±ãƒƒãƒˆã¸ã®æ¨©é™ã‚’ä»˜ä¸
resource "google_storage_bucket_iam_member" "storage_transfer_service_bucket" {
  bucket = google_storage_bucket.analytics_data.name
  role   = "roles/storage.admin"
  member = "serviceAccount:${data.google_storage_transfer_project_service_account.default.email}"
}

# S3 â†’ GCS è»¢é€ã‚¸ãƒ§ãƒ–
resource "google_storage_transfer_job" "s3_to_gcs_sync" {
  project     = var.gcp_project_id
  description = "Sync from AWS S3 to GCS every 3 hours"
  status      = "ENABLED"

  transfer_spec {
    # è»¢é€å…ƒ: AWS S3
    aws_s3_data_source {
      bucket_name = aws_s3_bucket.analytics_data.bucket
      role_arn    = aws_iam_role.gcp_sts_role.arn
    }

    # è»¢é€å…ˆ: GCS
    gcs_data_sink {
      bucket_name = google_storage_bucket.analytics_data.name
    }

    transfer_options {
      delete_objects_from_source_after_transfer  = false
      overwrite_objects_already_existing_in_sink = true
    }
  }

  schedule {
    schedule_start_date {
      year  = 2025
      month = 1
      day   = 1
    }
    start_time_of_day {
      hours   = 1
      minutes = 0
      seconds = 0
      nanos   = 0
    }
    # 3æ™‚é–“ã”ã¨ã«ç¹°ã‚Šè¿”ã—ï¼ˆ10800ç§’ï¼‰
    repeat_interval = "10800s"
  }
}
```

## BigQuery ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ»External Table

GCS ä¸Šã® Parquet ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã™ã‚‹ External Table ã‚’ä½œæˆã—ã¾ã™ã€‚

```hcl:bigquery.tf
# BigQuery ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
resource "google_bigquery_dataset" "analytics_data" {
  dataset_id    = "sample_aurora_sync"
  friendly_name = "Analytics Data from Aurora"
  description   = "Analytics data imported from Aurora database via DMS and GCS"
  location      = var.gcp_region
}

# å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«ã®å®šç¾©
locals {
  tables = [
    "users",
    "orders",
    "products",
    "categories",
    # å¿…è¦ãªãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¿½åŠ 
  ]
}

# å„ãƒ†ãƒ¼ãƒ–ãƒ«ã® External Table ã‚’ä½œæˆ
resource "google_bigquery_table" "external_tables" {
  for_each = toset(local.tables)

  dataset_id  = google_bigquery_dataset.analytics_data.dataset_id
  table_id    = each.key
  description = "External table for ${each.key} data from GCS"

  external_data_configuration {
    autodetect    = true
    source_format = "PARQUET"
    source_uris = [
      "gs://${google_storage_bucket.analytics_data.name}/myapp/${each.key}/LOAD00000001.parquet"
    ]

    parquet_options {
      enable_list_inference = true
      enum_as_string        = true
    }
  }
}

# BigQuery ç”¨ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
resource "google_service_account" "bigquery_service_account" {
  account_id   = "bigquery-analytics-sa"
  display_name = "BigQuery Analytics Service Account"
  description  = "Service account for BigQuery analytics data access"
}

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™
resource "google_bigquery_dataset_iam_member" "bigquery_dataset_access" {
  dataset_id = google_bigquery_dataset.analytics_data.dataset_id
  role       = "roles/bigquery.dataEditor"
  member     = "serviceAccount:${google_service_account.bigquery_service_account.email}"
}

# GCS ãƒã‚±ãƒƒãƒˆã‹ã‚‰ã®èª­ã¿å–ã‚Šæ¨©é™
resource "google_storage_bucket_iam_member" "bigquery_gcs_access" {
  bucket = google_storage_bucket.analytics_data.name
  role   = "roles/storage.objectViewer"
  member = "serviceAccount:${google_service_account.bigquery_service_account.email}"
}

# BigQuery Job User æ¨©é™ï¼ˆã‚¯ã‚¨ãƒªå®Ÿè¡Œç”¨ï¼‰
resource "google_project_iam_member" "bigquery_job_user" {
  project = var.gcp_project_id
  role    = "roles/bigquery.jobUser"
  member  = "serviceAccount:${google_service_account.bigquery_service_account.email}"
}

# BigQuery Data Viewer æ¨©é™ï¼ˆãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Šç”¨ï¼‰
resource "google_project_iam_member" "bigquery_data_viewer" {
  project = var.gcp_project_id
  role    = "roles/bigquery.dataViewer"
  member  = "serviceAccount:${google_service_account.bigquery_service_account.email}"
}
```

# é‹ç”¨ä¸Šã®æ³¨æ„ç‚¹

## Full-load ã®ãƒ‡ãƒ¼ã‚¿é‡

æœ¬æ§‹æˆã§ã¯ `full-load` ã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šã€æ¯å›ãƒ†ãƒ¼ãƒ–ãƒ«å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ã—ã¾ã™ã€‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„å ´åˆã€è»¢é€æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã™ã‚‹ãŸã‚æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚

## ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°å•é¡Œ

æœ¬ã‚µãƒ³ãƒ—ãƒ«ã§ã¯ EventBridge Schedulerï¼ˆJST åŸºæº–ï¼‰ã¨ Storage Transfer Jobï¼ˆUTC åŸºæº–ï¼‰ãŒç‹¬ç«‹ã—ã¦å‹•ä½œã—ã¦ã„ã¾ã™ã€‚ãã®ãŸã‚ã€DMS ã®å®Œäº†å‰ã« Storage Transfer ãŒèµ°ã‚‹ã¨ã€S3 ã®æœ€æ–°ãƒ‡ãƒ¼ã‚¿ãŒ GCS ã«åæ˜ ã•ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

**å¯¾ç­–ï¼š**

1. **ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ãšã‚‰ã™**: DMS å®Œäº†å¾Œã«ååˆ†ãªä½™è£•ã‚’æŒã£ã¦ Storage Transfer ã‚’é–‹å§‹ã™ã‚‹ï¼ˆä¾‹: DMS é–‹å§‹ã‹ã‚‰ 1 æ™‚é–“å¾Œï¼‰
2. **ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•ã«ã™ã‚‹**: DMS å®Œäº†ã‚’ EventBridge ã§æ¤œçŸ¥ã—ã€Lambda çµŒç”±ã§ Storage Transfer ã‚’é–‹å§‹ã™ã‚‹è¨­è¨ˆã«å¤‰æ›´ã™ã‚‹

# Full-load ã¨ CDCï¼ˆå·®åˆ†åŒæœŸï¼‰ã«ã¤ã„ã¦

DMS ã®ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ–¹å¼ã«ã¯ Full-load ã¨ CDCï¼ˆChange Data Captureï¼‰ãŒã‚ã‚Šã¾ã™ã€‚

## CDC ã®ãƒ¡ãƒªãƒƒãƒˆ

CDC ã¯æœ¬ç•ªé‹ç”¨ã§æ¨å¥¨ã•ã‚Œã‚‹æ–¹å¼ã§ã™ï¼š

- **åŠ¹ç‡çš„ãªè»¢é€**: å¤‰æ›´ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’è»¢é€ã™ã‚‹ãŸã‚ã€è»¢é€é‡ãŒå¤§å¹…ã«å‰Šæ¸›
- **é«˜ã„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§**: ç¶™ç¶šçš„ã«ãƒ‡ãƒ¼ã‚¿ã‚’åŒæœŸã—ã€ãƒ‹ã‚¢ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã®åˆ†æãŒå¯èƒ½
- **Aurora ã¸ã®è² è·è»½æ¸›**: å·®åˆ†ã®ã¿å–å¾—ã™ã‚‹ãŸã‚ã€ã‚½ãƒ¼ã‚¹ DB ã¸ã®è² è·ãŒä½ã„

## æœ¬ã‚µãƒ³ãƒ—ãƒ«ã§ã¯ Full-load ã‚’æ¡ç”¨

æœ¬ã‚µãƒ³ãƒ—ãƒ«ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚ `full-load` ã‚’è¨­å®šã—ã¦ã„ã¾ã™ã€‚

| é …ç›®            | Full-load      | CDC                  |
| --------------- | -------------- | -------------------- |
| è»¢é€ãƒ‡ãƒ¼ã‚¿      | æ¯å›å…¨é‡       | å¤‰æ›´åˆ†ã®ã¿           |
| ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§  | ä½ï¼ˆå®šæœŸå®Ÿè¡Œï¼‰ | é«˜ï¼ˆç¶™ç¶šçš„ï¼‰         |
| è¨­å®šã®è¤‡é›‘ã•    | ã‚·ãƒ³ãƒ—ãƒ«       | è¤‡é›‘                 |
| Aurora å´ã®è¦ä»¶ | ãªã—           | ãƒã‚¤ãƒŠãƒªãƒ­ã‚°ã®æœ‰åŠ¹åŒ– |

å®Ÿé‹ç”¨ã§ã¯ CDC ã®æ¡ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚CDC ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ `replication_type` ã‚’ `full-load-and-cdc` ã¾ãŸã¯ `cdc` ã«å¤‰æ›´ã—ã€Aurora å´ã§ãƒã‚¤ãƒŠãƒªãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

# ã¾ã¨ã‚

æœ¬è¨˜äº‹ã§ã¯ã€Terraform ã‚’ä½¿ç”¨ã—ã¦ Aurora ã‹ã‚‰ BigQuery ã¸ã®ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã‚’è§£èª¬ã—ã¾ã—ãŸã€‚

ä¸»ãªãƒã‚¤ãƒ³ãƒˆï¼š

- DMS Serverless ã§ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ãªãƒ‡ãƒ¼ã‚¿æŠ½å‡º
- Parquet å½¢å¼ã§åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ä¿å­˜
- OpenID Connect ãƒ•ã‚§ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚»ã‚­ãƒ¥ã‚¢ãªã‚¯ãƒ­ã‚¹ã‚¯ãƒ©ã‚¦ãƒ‰é€£æº
- External Table ã§ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ä¸è¦ã®åˆ†æåŸºç›¤

ã“ã®æ§‹æˆã«ã‚ˆã‚Šã€é‹ç”¨è² è·ã‚’æŠ‘ãˆãªãŒã‚‰ Aurora ã®ãƒ‡ãƒ¼ã‚¿ã‚’ BigQuery ã§åˆ†æã§ãã‚‹åŸºç›¤ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚

# å‚è€ƒãƒªãƒ³ã‚¯

- [AWS DMS Serverless](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Serverless.html)
- [Google Storage Transfer Service](https://cloud.google.com/storage-transfer/docs/overview)
- [BigQuery External Tables](https://cloud.google.com/bigquery/docs/external-tables)
- [AWS to GCP Workload Identity Federation](https://cloud.google.com/iam/docs/workload-identity-federation-with-other-clouds)
